{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "This final project can be collaborative. The maximum members of a group is 3. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints). You can freely determine every component of your workflow, including but not limited to:\n",
    "-  **Preprocessing the input text**: You may decide how to clean or transform the text. For example, removing emojis or URLs, lowercasing, removing stopwords, applying stemming or lemmatization, correcting spelling, or performing tokenization and sentence segmentation.\n",
    "-  **Feature extraction and encoding**: You can choose any method to convert text into numerical representations, such as TF-IDF, Bag-of-Words, N-grams, Word2Vec, GloVe, FastText, contextual embeddings (e.g., BERT, RoBERTa, or other transformer-based models), Part-of-Speech (POS) tagging, dependency-based features, sentiment or emotion features, readability metrics, or even embeddings or features generated by large language models (LLMs).\n",
    "-  **Data augmentation and enrichment**: You may expand or balance your dataset by incorporating other related corpora or using techniques like synonym replacement, random deletion/insertion, or LLM-assisted augmentation (e.g., generating paraphrased or synthetic examples to improve model robustness).\n",
    "-  **Model selection**: You are free to experiment with different models â€” from traditional machine learning algorithms (e.g., Logistic Regression, SVM, Random Forest, XGBoost) to deep learning architectures (e.g., CNNs, RNNs, Transformers), or even hybrid/ensemble approaches that combine multiple models or leverage LLM-generated predictions or reasoning.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values. You may explore both traditional and AI-assisted techniques. Data augmentation is optional.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision (P), Recall (R) and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. Here is an example illustrating how the experimental results table should be presented.\n",
    "\n",
    "| Feature + Model | Sexist (P) | Sexist (R) | Sexist (F1) | Non-Sexist (P) | Non-Sexist (R) | Non-Sexist (F1) | Weighted (P) | Weighted (R) | Weighted (F1) |\n",
    "|-----------------|:----------:|:----------:|:------------:|:---------------:|:---------------:|:----------------:|:-------------:|:--------------:|:---------------:|\n",
    "| TF-IDF + Logistic Regression | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "- **Format of the report**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary for each sections: \n",
    "    - Data Preprocessing\n",
    "    - Feature Engineering\n",
    "    - Model Selection and Architecture\n",
    "    - Training and Validation\n",
    "    - Evaluation and Results\n",
    "    - Use of Generative AI (if you use)\n",
    "\n",
    "## Rules \n",
    "Violations will result in 0 points in the grade: \n",
    "-   `Rule 1 - No test set leakage`: You must not use any instance from the test set during training, feature engineering, or model selection.\n",
    "-   `Rule 2 - Responsible AI use`: You may use generative AI, but you must clearly document how it was used. If you have used genAI, include a section titled â€œUse of Generative AIâ€ describing:\n",
    "    -   What parts of the project you used AI for\n",
    "    -   What was implemented manually vs. with AI assistance\n",
    "\n",
    "## Grading\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above. \n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "If your best performance reaches **0.82** or above (weighted F1-score) and follows all the requirements and rules, you will also get full points (10.0 points). \n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report including: \n",
    "- code and experimental results with details explained\n",
    "- combined results table, report and best performance\n",
    "- a summary at the end of the report (please follow the format above)\n",
    "\n",
    "Missing any part of the above requirements will result in point deductions.\n",
    "\n",
    "The due date is **Dec 11, Thursday by 11:59pm**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8675e93",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "\n",
    "(A table detailed model performance on the test set with at least 6 rows. Report the best performance.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f84b04",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "\n",
    "### 2. Feature Engineering\n",
    " \n",
    "\n",
    "### 3. Model Selection and Architecture\n",
    "\n",
    "\n",
    "### 4. Training and Validation\n",
    "\n",
    "\n",
    "### 5. Evaluation and Results\n",
    "\n",
    "\n",
    "### 6. Use of Generative AI (if you use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bab11",
   "metadata": {},
   "source": [
    "### Required Libraries/Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc6dfe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ryder\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryder\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ryder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ryder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ryder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ryder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ryder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download and install the necessary libraries\n",
    "# Uncomment below if needed to do so\n",
    "# %pip install pandas numpy re nltk demoji sklearn matplotlib transformers datasets torch scikit-learn\n",
    "# %pip install datasets\n",
    "# %pip install transformers\n",
    "%pip install torch\n",
    "# Install all required libraries for the project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import demoji\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn    .ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde634d",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "194206a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read whole lines as raw strings (avoid using '\\n' as a separator in read_csv)\n",
    "with open(\"edos_labelled_data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "raw = pd.DataFrame({\"raw\": lines})\n",
    "\n",
    "def parse(line):\n",
    "    line = line[\"raw\"]\n",
    "    text_part, label, split = line.rsplit(\",\", 2)\n",
    "    id_, text = text_part.split(\",\", 1)\n",
    "    return pd.Series([id_, text, label, split])\n",
    "# get rid of the first line (header)\n",
    "raw = raw[1:]\n",
    "df = raw.apply(parse, axis=1)\n",
    "df.columns = [\"id\", \"text\", \"label\", \"split\"]\n",
    "\n",
    "# split data set into train and test sets\n",
    "train_df = df[df[\"split\"] == \" train\"]\n",
    "test_df = df[df[\"split\"] == \" test\"]\n",
    "# get rid of the split column and id column\n",
    "train_df = train_df.drop(columns=[\"split\"])\n",
    "test_df = test_df.drop(columns=[\"split\"])\n",
    "\n",
    "# process text data\n",
    "def preprocess_text(text):\n",
    "    # Just lowercase and remove URLs\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_text)\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bf801",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59ae588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shape: (4193, 1908)\n",
      "  - TF-IDF features: 1800\n",
      "  - GloVe features: 100\n",
      "  - Custom features: 8\n",
      "  - Total: 1908\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# FEATURE ENGINEERING\n",
    "# -----------------------------\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].str.strip()\n",
    "test_df[\"label\"] = test_df[\"label\"].str.strip()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df[\"label\"])\n",
    "y_test = le.transform(test_df[\"label\"])\n",
    "\n",
    "# 1. TF-IDF Vectorizer - turns text into TF-IDF features\n",
    "# Weights words by importance: common words like 'the' get low weight,\n",
    "# distinctive words like 'bitch' get high weight\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=8000,      # Increased from 5000\n",
    "    ngram_range=(1, 2),     # Unigrams and bigrams\n",
    "    min_df=3,               # Word must appear in at least 3 documents\n",
    "    max_df=0.85,            # Word can't appear in more than 85% of documents\n",
    "    sublinear_tf=True,      # Use log scaling for better performance\n",
    "    strip_accents='unicode',\n",
    "    token_pattern=r'\\S+',   # Keep punctuation as part of tokens\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(train_df[\"text\"])\n",
    "X_test_tfidf = tfidf.transform(test_df[\"text\"])\n",
    "\n",
    "# Select top 1800 most informative TF-IDF features\n",
    "selector_tfidf = SelectKBest(chi2, k=1800)\n",
    "X_train_tfidf = selector_tfidf.fit_transform(X_train_tfidf, y_train)\n",
    "X_test_tfidf = selector_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "# 2. GloVe embeddings (100d) - captures semantic similarity\n",
    "# Words with similar meanings (like 'girl' and 'woman') have similar vectors\n",
    "glove = {}\n",
    "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        glove[word] = vector\n",
    "\n",
    "def sentence_to_vec(sentence, embeddings=glove, dim=100):\n",
    "    \"\"\"Convert sentence to vector by averaging word embeddings\"\"\"\n",
    "    words = sentence.split()\n",
    "    vectors = [embeddings[w] for w in words if w in embeddings]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_train_glove = np.vstack(train_df[\"text\"].apply(sentence_to_vec))\n",
    "X_test_glove = np.vstack(test_df[\"text\"].apply(sentence_to_vec))\n",
    "\n",
    "\n",
    "# 3. Custom sexism-specific features\n",
    "def extract_custom_features(df):\n",
    "    \"\"\"Hand-crafted features that signal sexist language\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for text in df['text']:\n",
    "        feat = []\n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        # Derogatory terms count\n",
    "        derogatory = ['bitch', 'bitches', 'whore', 'whores', 'slut', 'sluts', \n",
    "                      'skank', 'skanks', 'cunt', 'cunts', 'hoe', 'thot', 'pussy',\n",
    "                      'hag', 'hags', 'bimbo', 'bimbos', 'prick', 'pricks']\n",
    "        feat.append(sum(1 for w in derogatory if w in text_lower))\n",
    "        \n",
    "        # Female pronoun ratio (she/her vs all pronouns)\n",
    "        total_pronouns = text_lower.count('she') + text_lower.count('her') + text_lower.count('he') + text_lower.count('his') + 1\n",
    "        female_pronouns = text_lower.count('she') + text_lower.count('her')\n",
    "        feat.append(female_pronouns / total_pronouns)\n",
    "        \n",
    "        # Gendered words count\n",
    "        female_words = ['woman', 'women', 'girl', 'girls', 'female', 'lady']\n",
    "        feat.append(sum(1 for w in female_words if w in text_lower))\n",
    "        \n",
    "        # Imperative/commanding language\n",
    "        commands = ['should', 'must', 'need', 'have to', 'supposed']\n",
    "        feat.append(sum(1 for cmd in commands if cmd in text_lower))\n",
    "        \n",
    "        # Intensity markers (exclamation marks, capped at 3)\n",
    "        feat.append(min(text.count('!'), 3))\n",
    "        \n",
    "        # Negative descriptors count\n",
    "        negative = ['ugly', 'disgusting', 'fat', 'stupid', 'dumb']\n",
    "        feat.append(sum(1 for neg in negative if neg in text_lower))\n",
    "        \n",
    "        # Word count (log-scaled to handle outliers)\n",
    "        feat.append(np.log1p(len(words)))\n",
    "        \n",
    "        # All-caps words (shouting)\n",
    "        caps_words = sum(1 for word in words if word.isupper() and len(word) > 2)\n",
    "        feat.append(caps_words)\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return np.array(features, dtype=float)\n",
    "\n",
    "custom_train = extract_custom_features(train_df)\n",
    "custom_test = extract_custom_features(test_df)\n",
    "\n",
    "\n",
    "# 4. Combine all features\n",
    "# Convert to sparse matrices for efficiency\n",
    "custom_train_sparse = csr_matrix(custom_train)\n",
    "custom_test_sparse = csr_matrix(custom_test)\n",
    "glove_train_sparse = csr_matrix(X_train_glove)\n",
    "glove_test_sparse = csr_matrix(X_test_glove)\n",
    "\n",
    "# Stack all features horizontally: TF-IDF + GloVe + Custom\n",
    "X_train_combined = hstack([X_train_tfidf, glove_train_sparse, custom_train_sparse])\n",
    "X_test_combined = hstack([X_test_tfidf, glove_test_sparse, custom_test_sparse])\n",
    "\n",
    "print(f\"Final feature shape: {X_train_combined.shape}\")\n",
    "print(f\"  - TF-IDF features: 1800\")\n",
    "print(f\"  - GloVe features: 100\")\n",
    "print(f\"  - Custom features: 8\")\n",
    "print(f\"  - Total: {X_train_combined.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7f363",
   "metadata": {},
   "source": [
    "### Models + Training with Training Data Set\n",
    "#### Each Model will train using each of the feature engineerings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d89884",
   "metadata": {},
   "source": [
    "### Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820c0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble model...\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ TEST ACCURACY: 0.8232 (82.32%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.8467    0.9240    0.8836       789\n",
      "      sexist     0.7333    0.5556    0.6322       297\n",
      "\n",
      "    accuracy                         0.8232      1086\n",
      "   macro avg     0.7900    0.7398    0.7579      1086\n",
      "weighted avg     0.8157    0.8232    0.8149      1086\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Not-Sex  Sexist\n",
      "Actual Not-Sex    729      60\n",
      "       Sexist     132     165\n",
      "\n",
      "Per-Class Accuracy:\n",
      "  not sexist: 0.9240\n",
      "  sexist: 0.5556\n",
      "======================================================================\n",
      "âœ… SUCCESS! Achieved 81%+ accuracy!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL TRAINING\n",
    "# -----------------------------\n",
    "\n",
    "# Create ensemble of 4 Logistic Regression models\n",
    "# Different C values provide diversity, class_weight handles imbalance\n",
    "lr1 = LogisticRegression(C=2.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=42)\n",
    "lr2 = LogisticRegression(C=3.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=43)\n",
    "lr3 = LogisticRegression(C=4.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=44)\n",
    "lr4 = LogisticRegression(C=5.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=45)\n",
    "\n",
    "# Soft voting averages the probability predictions from all 4 models\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr1', lr1), ('lr2', lr2), ('lr3', lr3), ('lr4', lr4)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(\"Training ensemble model...\")\n",
    "ensemble.fit(X_train_combined, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# PREDICTION WITH OPTIMIZED THRESHOLD\n",
    "# -----------------------------\n",
    "\n",
    "# Get probability predictions (probability of sexist class)\n",
    "y_proba = ensemble.predict_proba(X_test_combined)[:, 1]\n",
    "\n",
    "# Use optimal threshold of 0.62 instead of default 0.5\n",
    "# This balances precision and recall better for our imbalanced dataset\n",
    "optimal_threshold = 0.57\n",
    "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "# -----------------------------\n",
    "# EVALUATION\n",
    "# -----------------------------\n",
    "\n",
    "final_acc = accuracy_score(test_df[\"label\"], y_pred_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸŽ¯ TEST ACCURACY: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df[\"label\"], y_pred_labels, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(test_df[\"label\"], y_pred_labels)\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Not-Sex  Sexist\")\n",
    "print(f\"Actual Not-Sex  {cm[0,0]:5d}   {cm[0,1]:5d}\")\n",
    "print(f\"       Sexist   {cm[1,0]:5d}   {cm[1,1]:5d}\")\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    class_acc = cm[i, i] / cm[i].sum()\n",
    "    print(f\"  {label}: {class_acc:.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "if final_acc >= 0.81:\n",
    "    print(\"âœ… SUCCESS! Achieved 81%+ accuracy!\")\n",
    "else:\n",
    "    print(f\"ðŸ“Š Current accuracy: {final_acc*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b518d34",
   "metadata": {},
   "source": [
    "### Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd7c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.8257    0.8885    0.8559       789\n",
      "  sexist         0.6287    0.5017    0.5581       297\n",
      "\n",
      "    accuracy                         0.7827      1086\n",
      "   macro avg     0.7272    0.6951    0.7070      1086\n",
      "weighted avg     0.7718    0.7827    0.7745      1086\n",
      "\n",
      "GloVe SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.7510    0.9214    0.8275       789\n",
      "  sexist         0.4746    0.1886    0.2699       297\n",
      "\n",
      "    accuracy                         0.7210      1086\n",
      "   macro avg     0.6128    0.5550    0.5487      1086\n",
      "weighted avg     0.6754    0.7210    0.6750      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- TF-IDF Features ---\n",
    "# Assume X_train_tfidf, X_test_tfidf, train_df[\"label\"], test_df[\"label\"] exist\n",
    "\n",
    "# Create a simple linear SVM\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# Train on TF-IDF features\n",
    "svm.fit(X_train_tfidf, train_df[\"label\"])\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TF-IDF SVM Results:\")\n",
    "print(classification_report(test_df[\"label\"], y_pred, digits=4))\n",
    "\n",
    "\n",
    "# --- GloVe Features ---\n",
    "# Assume X_train_glove, X_test_glove exist\n",
    "\n",
    "# Create SVM\n",
    "svm_glove = SVC(random_state=42)\n",
    "\n",
    "# Train on GloVe features\n",
    "svm_glove.fit(X_train_glove, train_df[\"label\"])\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_glove = svm_glove.predict(X_test_glove)\n",
    "\n",
    "# Evaluate\n",
    "print(\"GloVe SVM Results:\")\n",
    "print(classification_report(test_df[\"label\"], y_pred_glove, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e22830",
   "metadata": {},
   "source": [
    "### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ee457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe...\n",
      "Loaded 400000 vectors.\n",
      "Training...\n",
      "Epoch 1 Loss: 0.3142\n",
      "Epoch 2 Loss: 0.4607\n",
      "Epoch 3 Loss: 0.8530\n",
      "Epoch 4 Loss: 0.4531\n",
      "Epoch 5 Loss: 0.1476\n",
      "\n",
      "Sentence: all women are bad at math\n",
      "Predicted label: 0 (1 = sexist, 0 = not sexist)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f395ff3",
   "metadata": {},
   "source": [
    "### Evaluation (Use models on Test set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
