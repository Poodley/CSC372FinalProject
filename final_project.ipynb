{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "This final project can be collaborative. The maximum members of a group is 3. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints). You can freely determine every component of your workflow, including but not limited to:\n",
    "-  **Preprocessing the input text**: You may decide how to clean or transform the text. For example, removing emojis or URLs, lowercasing, removing stopwords, applying stemming or lemmatization, correcting spelling, or performing tokenization and sentence segmentation.\n",
    "-  **Feature extraction and encoding**: You can choose any method to convert text into numerical representations, such as TF-IDF, Bag-of-Words, N-grams, Word2Vec, GloVe, FastText, contextual embeddings (e.g., BERT, RoBERTa, or other transformer-based models), Part-of-Speech (POS) tagging, dependency-based features, sentiment or emotion features, readability metrics, or even embeddings or features generated by large language models (LLMs).\n",
    "-  **Data augmentation and enrichment**: You may expand or balance your dataset by incorporating other related corpora or using techniques like synonym replacement, random deletion/insertion, or LLM-assisted augmentation (e.g., generating paraphrased or synthetic examples to improve model robustness).\n",
    "-  **Model selection**: You are free to experiment with different models — from traditional machine learning algorithms (e.g., Logistic Regression, SVM, Random Forest, XGBoost) to deep learning architectures (e.g., CNNs, RNNs, Transformers), or even hybrid/ensemble approaches that combine multiple models or leverage LLM-generated predictions or reasoning.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values. You may explore both traditional and AI-assisted techniques. Data augmentation is optional.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision (P), Recall (R) and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. Here is an example illustrating how the experimental results table should be presented.\n",
    "\n",
    "| Feature + Model | Sexist (P) | Sexist (R) | Sexist (F1) | Non-Sexist (P) | Non-Sexist (R) | Non-Sexist (F1) | Weighted (P) | Weighted (R) | Weighted (F1) |\n",
    "|-----------------|:----------:|:----------:|:------------:|:---------------:|:---------------:|:----------------:|:-------------:|:--------------:|:---------------:|\n",
    "| TF-IDF + Logistic Regression | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "- **Format of the report**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary for each sections: \n",
    "    - Data Preprocessing\n",
    "    - Feature Engineering\n",
    "    - Model Selection and Architecture\n",
    "    - Training and Validation\n",
    "    - Evaluation and Results\n",
    "    - Use of Generative AI (if you use)\n",
    "\n",
    "## Rules \n",
    "Violations will result in 0 points in the grade: \n",
    "-   `Rule 1 - No test set leakage`: You must not use any instance from the test set during training, feature engineering, or model selection.\n",
    "-   `Rule 2 - Responsible AI use`: You may use generative AI, but you must clearly document how it was used. If you have used genAI, include a section titled “Use of Generative AI” describing:\n",
    "    -   What parts of the project you used AI for\n",
    "    -   What was implemented manually vs. with AI assistance\n",
    "\n",
    "## Grading\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above. \n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "If your best performance reaches **0.82** or above (weighted F1-score) and follows all the requirements and rules, you will also get full points (10.0 points). \n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report including: \n",
    "- code and experimental results with details explained\n",
    "- combined results table, report and best performance\n",
    "- a summary at the end of the report (please follow the format above)\n",
    "\n",
    "Missing any part of the above requirements will result in point deductions.\n",
    "\n",
    "The due date is **Dec 11, Thursday by 11:59pm**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8675e93",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "\n",
    "(A table detailed model performance on the test set with at least 6 rows. Report the best performance.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f84b04",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "\n",
    "### 2. Feature Engineering\n",
    " \n",
    "\n",
    "### 3. Model Selection and Architecture\n",
    "\n",
    "\n",
    "### 4. Training and Validation\n",
    "\n",
    "\n",
    "### 5. Evaluation and Results\n",
    "\n",
    "\n",
    "### 6. Use of Generative AI (if you use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bab11",
   "metadata": {},
   "source": [
    "### Required Libraries/Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6dfe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slueb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Download and install the necessary libraries\n",
    "# Uncomment below if needed to do so\n",
    "#%pip install numpy\n",
    "#%pip install pandas\n",
    "#%pip install sklearn\n",
    "#%pip install scipy\n",
    "#%pip install sentence-transformers\n",
    "# Install all required libraries for the project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC as SVM\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    ")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde634d",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "Here in data preprocessing, we are mainly grabbing the CSV data and correctly parsing it using our parse() function. Due to the text portion within the csv having commas, we must split the id from the left and the label along with split column from the right and the remaining portion would be our text portion. We then use pandas Dataframe filtering to split the data set into the training and testing sets. For text preprocessing, we decided to utilize the re library to get rid of URLs and extra spacing within text as well as Python's .lower() function. After we applied the preprocessing function to the data set. We stripped the label texts as well as put them in a LabelEncoder to ease the use for our machine learning classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194206a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open the csv\n",
    "with open(\"edos_labelled_data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "# place into a dataframe and label them as raw\n",
    "raw = pd.DataFrame({\"raw\": lines})\n",
    "\n",
    "def parse(line):\n",
    "    # parse each line into id, text, label, split\n",
    "    line = line[\"raw\"]\n",
    "    # split from the right to get label and split\n",
    "    text_and_id_part, label, split = line.rsplit(\",\", 2)\n",
    "    # split from the left to get and text\n",
    "    id_, text = text_and_id_part.split(\",\", 1)\n",
    "    return pd.Series([id_, text, label, split])\n",
    "# get rid of the first line (header)\n",
    "raw = raw[1:]\n",
    "# apply the parse function to each row\n",
    "df = raw.apply(parse, axis=1)\n",
    "# label the columns\n",
    "df.columns = [\"id\", \"text\", \"label\", \"split\"]\n",
    "\n",
    "# split data set into train and test sets\n",
    "train_df = df[df[\"split\"] == \" train\"]\n",
    "test_df = df[df[\"split\"] == \" test\"]\n",
    "# get rid of the split column and id column\n",
    "train_df = train_df.drop(columns=[\"split\"])\n",
    "test_df = test_df.drop(columns=[\"split\"])\n",
    "\n",
    "# process text data\n",
    "def preprocess_text(text):\n",
    "    # Just lowercase and remove URLs\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "# apply preprocessing function to text data\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_text)\n",
    "# strip whitespace from labels\n",
    "train_df[\"label\"] = train_df[\"label\"].str.strip()\n",
    "test_df[\"label\"] = test_df[\"label\"].str.strip()\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df[\"label\"])\n",
    "y_test = le.transform(test_df[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bf801",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering\n",
    "[Insert TF-IDF Vectorizer and Glove embeddings explanation here].\n",
    "We also decided to use \"all-mpnet-base-v2\" model from contextual embeddings with the Sentence-Transformers library. We also added our own function to flag slurs and general derogatory statements towards women. After testing with these features individually, we were unable to get satisfactory results so we seeked assistance from AI to see if there was a way to combine these features to create an ultimate feature engineered set which is done by putting the data sets trained by each feature into a csr_matrix which is then combined into our TF-IDF Vectorizer using the hstack function from scipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slueb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\slueb\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual embedding shape: (4193, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. TF-IDF Vectorizer - turns text into TF-IDF features\n",
    "# Weights words by importance: common words like 'the' get low weight,\n",
    "# distinctive words like 'bitch' get high weight\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=8000,      # Increased from 5000\n",
    "    ngram_range=(1, 2),     # Unigrams and bigrams\n",
    "    min_df=3,               # Word must appear in at least 3 documents\n",
    "    max_df=0.85,            # Word can't appear in more than 85% of documents\n",
    "    sublinear_tf=True,      # Use log scaling for better performance\n",
    "    strip_accents='unicode',\n",
    "    token_pattern=r'\\S+',   # Keep punctuation as part of tokens\n",
    ")\n",
    "\n",
    "# apply vectorizer to a data set\n",
    "X_train_tfidf = tfidf.fit_transform(train_df[\"text\"])\n",
    "X_test_tfidf = tfidf.transform(test_df[\"text\"])\n",
    "\n",
    "# Select top 1800 most informative TF-IDF features\n",
    "selector_tfidf = SelectKBest(chi2, k=1800)\n",
    "X_train_tfidf = selector_tfidf.fit_transform(X_train_tfidf, y_train)\n",
    "X_test_tfidf = selector_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "# 2. GloVe embeddings (100d) - captures semantic similarity\n",
    "# Words with similar meanings (like 'girl' and 'woman') have similar vectors\n",
    "glove = {}\n",
    "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        glove[word] = vector\n",
    "\n",
    "def sentence_to_vec(sentence, embeddings=glove, dim=100):\n",
    "    \"\"\"Convert sentence to vector by averaging word embeddings\"\"\"\n",
    "    words = sentence.split()\n",
    "    vectors = [embeddings[w] for w in words if w in embeddings]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_train_glove = np.vstack(train_df[\"text\"].apply(sentence_to_vec))\n",
    "X_test_glove = np.vstack(test_df[\"text\"].apply(sentence_to_vec))\n",
    "\n",
    "# 3. CONTEXTUAL EMBEDDINGS (Sentence-BERT / MPNet)\n",
    "ctx_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "X_train_ctx = ctx_model.encode(\n",
    "    train_df[\"text\"].tolist(),\n",
    "    convert_to_numpy=True,\n",
    ")\n",
    "X_test_ctx = ctx_model.encode(\n",
    "    test_df[\"text\"].tolist(),\n",
    "    convert_to_numpy=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Contextual embedding shape:\", X_train_ctx.shape)\n",
    "\n",
    "\n",
    "# 4. Custom sexism-specific features\n",
    "def extract_custom_features(df):\n",
    "    \"\"\"Hand-crafted features that signal sexist language\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for text in df['text']:\n",
    "        feat = []\n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        # Derogatory terms count\n",
    "        derogatory = ['bitch', 'bitches', 'whore', 'whores', 'slut', 'sluts', \n",
    "                      'skank', 'skanks', 'cunt', 'cunts', 'hoe', 'thot', 'pussy',\n",
    "                      'hag', 'hags', 'bimbo', 'bimbos', 'prick', 'pricks']\n",
    "        feat.append(sum(1 for w in derogatory if w in text_lower))\n",
    "        \n",
    "        # Female pronoun ratio (she/her vs all pronouns)\n",
    "        total_pronouns = text_lower.count('she') + text_lower.count('her') + text_lower.count('he') + text_lower.count('his') + 1\n",
    "        female_pronouns = text_lower.count('she') + text_lower.count('her')\n",
    "        feat.append(female_pronouns / total_pronouns)\n",
    "        \n",
    "        # Gendered words count\n",
    "        female_words = ['woman', 'women', 'girl', 'girls', 'female', 'lady']\n",
    "        feat.append(sum(1 for w in female_words if w in text_lower))\n",
    "        \n",
    "        # Imperative/commanding language\n",
    "        commands = ['should', 'must', 'need', 'have to', 'supposed']\n",
    "        feat.append(sum(1 for cmd in commands if cmd in text_lower))\n",
    "        \n",
    "        # Intensity markers (exclamation marks, capped at 3)\n",
    "        feat.append(min(text.count('!'), 3))\n",
    "        \n",
    "        # Negative descriptors count\n",
    "        negative = ['ugly', 'disgusting', 'fat', 'stupid', 'dumb']\n",
    "        feat.append(sum(1 for neg in negative if neg in text_lower))\n",
    "        \n",
    "        # Word count (log-scaled to handle outliers)\n",
    "        feat.append(np.log1p(len(words)))\n",
    "        \n",
    "        # All-caps words (shouting)\n",
    "        caps_words = sum(1 for word in words if word.isupper() and len(word) > 2)\n",
    "        feat.append(caps_words)\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return np.array(features, dtype=float)\n",
    "\n",
    "custom_train = extract_custom_features(train_df)\n",
    "custom_test = extract_custom_features(test_df)\n",
    "\n",
    "\n",
    "# 4. Combine all features\n",
    "# Convert to sparse matrices for efficiency\n",
    "# Convert to sparse matrix so it can be stacked with TF-IDF\n",
    "glove_train_sparse = csr_matrix(X_train_glove)\n",
    "glove_test_sparse = csr_matrix(X_test_glove)\n",
    "ctx_train_sparse = csr_matrix(X_train_ctx)\n",
    "ctx_test_sparse = csr_matrix(X_test_ctx)\n",
    "custom_train_sparse = csr_matrix(custom_train)\n",
    "custom_test_sparse = csr_matrix(custom_test)\n",
    "\n",
    "# Stack all features horizontally: TF-IDF + GloVe + Custom\n",
    "X_train_combined = hstack([\n",
    "    X_train_tfidf,\n",
    "    glove_train_sparse,\n",
    "    ctx_train_sparse,\n",
    "    custom_train_sparse\n",
    "])\n",
    "\n",
    "X_test_combined = hstack([\n",
    "    X_test_tfidf,\n",
    "    glove_test_sparse,\n",
    "    ctx_test_sparse,\n",
    "    custom_test_sparse\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7f363",
   "metadata": {},
   "source": [
    "### Models + Training with Training Data Set\n",
    "For our models we decided to use the general machine learning models, practiced in the Homeworks, LogisticRegression and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d89884",
   "metadata": {},
   "source": [
    "### Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820c0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble model...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.8453    0.8657    0.8554       789\n",
      "      sexist     0.6187    0.5791    0.5983       297\n",
      "\n",
      "    accuracy                         0.7873      1086\n",
      "   macro avg     0.7320    0.7224    0.7268      1086\n",
      "weighted avg     0.7833    0.7873    0.7850      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create ensemble of 4 Logistic Regression models\n",
    "# Different C values provide diversity, class_weight handles imbalance\n",
    "lr1 = LogisticRegression(C=2.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=42)\n",
    "lr2 = LogisticRegression(C=3.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=43)\n",
    "lr3 = LogisticRegression(C=4.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=44)\n",
    "lr4 = LogisticRegression(C=5.0, class_weight={0: 1, 1: 1.5}, max_iter=2000, random_state=45)\n",
    "\n",
    "# Soft voting averages the probability predictions from all 4 models\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr1', lr1), ('lr2', lr2), ('lr3', lr3), ('lr4', lr4)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(\"Training ensemble model...\")\n",
    "ensemble.fit(X_train_combined, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# PREDICTION WITH OPTIMIZED THRESHOLD\n",
    "# -----------------------------\n",
    "\n",
    "# Get probability predictions (probability of sexist class)\n",
    "y_proba = ensemble.predict_proba(X_test_combined)[:, 1]\n",
    "\n",
    "# Use optimal threshold of 0.62 instead of default 0.5\n",
    "# This balances precision and recall better for our imbalanced dataset\n",
    "optimal_threshold = 0.57\n",
    "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "# -----------------------------\n",
    "# EVALUATION\n",
    "# -----------------------------\n",
    "\n",
    "print(classification_report(test_df[\"label\"], y_pred_labels, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe65b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GloVe-only model...\n",
      "\n",
      "=== GloVe-Only Logistic Regression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.7439    0.8872    0.8092       789\n",
      "      sexist     0.3862    0.1886    0.2534       297\n",
      "\n",
      "    accuracy                         0.6961      1086\n",
      "   macro avg     0.5650    0.5379    0.5313      1086\n",
      "weighted avg     0.6461    0.6961    0.6572      1086\n",
      "\n",
      "Training CTX model...\n",
      "\n",
      "=== ctx-Only Logistic Regression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.8455    0.8669    0.8561       789\n",
      "      sexist     0.6209    0.5791    0.5993       297\n",
      "\n",
      "    accuracy                         0.7882      1086\n",
      "   macro avg     0.7332    0.7230    0.7277      1086\n",
      "weighted avg     0.7841    0.7882    0.7858      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Glove only Logistic Regression model for comparison\n",
    "\n",
    "lr_glove = LogisticRegression(\n",
    "    C=3.0,\n",
    "    class_weight={0: 1, 1: 1.5},\n",
    "    max_iter=2000,\n",
    "    random_state=99\n",
    ")\n",
    "\n",
    "print(\"Training GloVe-only model...\")\n",
    "lr_glove.fit(X_train_glove, y_train)\n",
    "\n",
    "y_proba_glove = lr_glove.predict_proba(X_test_glove)[:, 1]\n",
    "\n",
    "y_pred_glove = (y_proba_glove >= optimal_threshold).astype(int)\n",
    "y_pred_glove_labels = le.inverse_transform(y_pred_glove)\n",
    "\n",
    "print(\"\\n=== GloVe-Only Logistic Regression ===\")\n",
    "print(classification_report(test_df[\"label\"], y_pred_glove_labels, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b518d34",
   "metadata": {},
   "source": [
    "### Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + GloVe + Contextual + Custom Features SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.8622    0.9037    0.8824       789\n",
      "      sexist     0.7066    0.6162    0.6583       297\n",
      "\n",
      "    accuracy                         0.8250      1086\n",
      "   macro avg     0.7844    0.7599    0.7703      1086\n",
      "weighted avg     0.8196    0.8250    0.8211      1086\n",
      "\n",
      "GloVe SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.7334    0.9240    0.8177       789\n",
      "      sexist     0.3478    0.1077    0.1645       297\n",
      "\n",
      "    accuracy                         0.7007      1086\n",
      "   macro avg     0.5406    0.5158    0.4911      1086\n",
      "weighted avg     0.6280    0.7007    0.6391      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a simple linear SVM\n",
    "svm = SVM(random_state=42)\n",
    "\n",
    "# Train on TF-IDF features\n",
    "svm.fit(X_train_combined,train_df[\"label\"])\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = svm.predict(X_test_combined)\n",
    "\n",
    "# Evaluate\n",
    "print(\"TF-IDF + GloVe + Contextual + Custom Features SVM Results:\")\n",
    "print(classification_report(test_df[\"label\"], y_pred, digits=4))\n",
    "\n",
    "\n",
    "# --- GloVe Features ---\n",
    "# Assume X_train_glove, X_test_glove exist\n",
    "\n",
    "# Create SVM\n",
    "svm_glove = SVM(random_state=42)\n",
    "\n",
    "# Train on GloVe features\n",
    "svm_glove.fit(X_train_glove, train_df[\"label\"])\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_glove = svm_glove.predict(X_test_glove)\n",
    "\n",
    "# Evaluate\n",
    "print(\"GloVe SVM Results:\")\n",
    "print(classification_report(test_df[\"label\"], y_pred_glove, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e22830",
   "metadata": {},
   "source": [
    "### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ee457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.7622    0.9873    0.8603       789\n",
      "      sexist     0.8438    0.1818    0.2992       297\n",
      "\n",
      "    accuracy                         0.7670      1086\n",
      "   macro avg     0.8030    0.5846    0.5797      1086\n",
      "weighted avg     0.7845    0.7670    0.7068      1086\n",
      "\n",
      "Random Forest (GloVe) Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.7370    0.9911    0.8454       789\n",
      "      sexist     0.7200    0.0606    0.1118       297\n",
      "\n",
      "    accuracy                         0.7366      1086\n",
      "   macro avg     0.7285    0.5259    0.4786      1086\n",
      "weighted avg     0.7324    0.7366    0.6448      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=2000,\n",
    "    class_weight={0: 1, 1: 1.5},\n",
    "    random_state=42\n",
    ")\n",
    "# Train on combined features (use encoded labels y_train)\n",
    "rf.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test_combined)\n",
    "# Convert predicted integer labels back to original string labels\n",
    "y_pred_rf_labels = le.inverse_transform(y_pred_rf)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(classification_report(test_df[\"label\"], y_pred_rf_labels, digits=4))\n",
    "\n",
    "rf_glove = RandomForestClassifier(\n",
    "    n_estimators=2000,\n",
    "    class_weight={0: 1, 1: 1.5},\n",
    "    random_state=42\n",
    ")\n",
    "# Train on GloVe features (use encoded labels y_train)\n",
    "rf_glove.fit(X_train_glove, y_train)\n",
    "\n",
    "# Predict on GloVe test set\n",
    "y_pred_rf_glove = rf_glove.predict(X_test_glove)\n",
    "y_pred_rf_glove_labels = le.inverse_transform(y_pred_rf_glove)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest (GloVe) Results:\")\n",
    "print(classification_report(test_df[\"label\"], y_pred_rf_glove_labels, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f395ff3",
   "metadata": {},
   "source": [
    "### Evaluation (Use models on Test set)\n",
    "create a dataframe with rows indicating feature+model and columns indicating Precision (P), Recall (R) and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. Here is an example illustrating how the experimental results table should be presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67cb180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature + Model</th>\n",
       "      <th>Sexist (P)</th>\n",
       "      <th>Sexist (R)</th>\n",
       "      <th>Sexist (F1)</th>\n",
       "      <th>Non-Sexist (P)</th>\n",
       "      <th>Non-Sexist (R)</th>\n",
       "      <th>Non-Sexist (F1)</th>\n",
       "      <th>Weighted (P)</th>\n",
       "      <th>Weighted (R)</th>\n",
       "      <th>Weighted (F1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Combined + LR</td>\n",
       "      <td>0.717472</td>\n",
       "      <td>0.649832</td>\n",
       "      <td>0.681979</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>0.903676</td>\n",
       "      <td>0.887920</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.831599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combined + SVM</td>\n",
       "      <td>0.706564</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>0.862152</td>\n",
       "      <td>0.903676</td>\n",
       "      <td>0.882426</td>\n",
       "      <td>0.819602</td>\n",
       "      <td>0.825046</td>\n",
       "      <td>0.821124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Combined + Random Forest</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.299169</td>\n",
       "      <td>0.762231</td>\n",
       "      <td>0.987326</td>\n",
       "      <td>0.860298</td>\n",
       "      <td>0.784525</td>\n",
       "      <td>0.767035</td>\n",
       "      <td>0.706840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe + LR</td>\n",
       "      <td>0.386207</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.743889</td>\n",
       "      <td>0.887199</td>\n",
       "      <td>0.809249</td>\n",
       "      <td>0.646070</td>\n",
       "      <td>0.696133</td>\n",
       "      <td>0.657233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GloVe + SVM</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.107744</td>\n",
       "      <td>0.164524</td>\n",
       "      <td>0.733400</td>\n",
       "      <td>0.923954</td>\n",
       "      <td>0.817723</td>\n",
       "      <td>0.627953</td>\n",
       "      <td>0.700737</td>\n",
       "      <td>0.639086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GloVe + Random Forest</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.737041</td>\n",
       "      <td>0.991128</td>\n",
       "      <td>0.845405</td>\n",
       "      <td>0.732380</td>\n",
       "      <td>0.736648</td>\n",
       "      <td>0.644779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature + Model  Sexist (P)  Sexist (R)  Sexist (F1)  \\\n",
       "0             Combined + LR    0.717472    0.649832     0.681979   \n",
       "1            Combined + SVM    0.706564    0.616162     0.658273   \n",
       "2  Combined + Random Forest    0.843750    0.181818     0.299169   \n",
       "3                GloVe + LR    0.386207    0.188552     0.253394   \n",
       "4               GloVe + SVM    0.347826    0.107744     0.164524   \n",
       "5     GloVe + Random Forest    0.720000    0.060606     0.111801   \n",
       "\n",
       "   Non-Sexist (P)  Non-Sexist (R)  Non-Sexist (F1)  Weighted (P)  \\\n",
       "0        0.872705        0.903676         0.887920      0.830252   \n",
       "1        0.862152        0.903676         0.882426      0.819602   \n",
       "2        0.762231        0.987326         0.860298      0.784525   \n",
       "3        0.743889        0.887199         0.809249      0.646070   \n",
       "4        0.733400        0.923954         0.817723      0.627953   \n",
       "5        0.737041        0.991128         0.845405      0.732380   \n",
       "\n",
       "   Weighted (R)  Weighted (F1)  \n",
       "0      0.834254       0.831599  \n",
       "1      0.825046       0.821124  \n",
       "2      0.767035       0.706840  \n",
       "3      0.696133       0.657233  \n",
       "4      0.700737       0.639086  \n",
       "5      0.736648       0.644779  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performance Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Feature + Model    Combined + LR\n",
       "Sexist (P)              0.717472\n",
       "Sexist (R)              0.649832\n",
       "Sexist (F1)             0.681979\n",
       "Non-Sexist (P)          0.872705\n",
       "Non-Sexist (R)          0.903676\n",
       "Non-Sexist (F1)          0.88792\n",
       "Weighted (P)            0.830252\n",
       "Weighted (R)            0.834254\n",
       "Weighted (F1)           0.831599\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a dataframe with rows indicating feature+model and columns indicating Precision (P), Recall (R) and F1-score (using weighted average). \n",
    "# Your results should have at least 6 rows (2 feature engineering methods x 3 models). \n",
    "# Report best performance with (1) your feature engineering method, and (2) the model you choose. Here is an example illustrating how the experimental results table should be presented.\n",
    "\n",
    "def extract_full_metrics(y_true, y_pred, feature_name, model_name, labels=[1,0]):\n",
    "    # labels ordered as: Sexist = 1, Non-Sexist = 0\n",
    "    precisions, recalls, f1s, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, zero_division=0\n",
    "    )\n",
    "    \n",
    "    weighted_p = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    weighted_r = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"Feature + Model\": f\"{feature_name} + {model_name}\",\n",
    "\n",
    "        \"Sexist (P)\": precisions[0],\n",
    "        \"Sexist (R)\": recalls[0],\n",
    "        \"Sexist (F1)\": f1s[0],\n",
    "\n",
    "        \"Non-Sexist (P)\": precisions[1],\n",
    "        \"Non-Sexist (R)\": recalls[1],\n",
    "        \"Non-Sexist (F1)\": f1s[1],\n",
    "\n",
    "        \"Weighted (P)\": weighted_p,\n",
    "        \"Weighted (R)\": weighted_r,\n",
    "        \"Weighted (F1)\": weighted_f1\n",
    "    }\n",
    "\n",
    "results = []\n",
    "# 1. Logistic Regression on COMBINED FEATURES\n",
    "y_proba_lr = ensemble.predict_proba(X_test_combined)[:, 1]\n",
    "y_pred_lr = (y_proba_lr >= 0.57).astype(int)\n",
    "results.append(extract_full_metrics(y_test, y_pred_lr, \"Combined\", \"LR\"))\n",
    "\n",
    "# 2. SVM on COMBINED FEATURES\n",
    "y_pred_svm = svm.predict(X_test_combined)\n",
    "y_pred_svm_int = le.transform(y_pred_svm)\n",
    "results.append(extract_full_metrics(y_test, y_pred_svm_int, \"Combined\", \"SVM\"))\n",
    "\n",
    "# 3. Random Forest on COMBINED FEATURES\n",
    "y_pred_rf = rf.predict(X_test_combined)\n",
    "results.append(extract_full_metrics(y_test, y_pred_rf, \"Combined\", \"Random Forest\"))\n",
    "\n",
    "\n",
    "# 4. Logistic Regression on GLOVE ONLY\n",
    "y_proba_lr_glove = lr_glove.predict_proba(X_test_glove)[:, 1]\n",
    "y_pred_lr_glove = (y_proba_lr_glove >= 0.57).astype(int)\n",
    "results.append(extract_full_metrics(y_test, y_pred_lr_glove, \"GloVe\", \"LR\"))\n",
    "\n",
    "# 5. SVM on GLOVE ONLY\n",
    "y_pred_svm_glove = svm_glove.predict(X_test_glove)\n",
    "y_pred_svm_glove_int = le.transform(y_pred_svm_glove)\n",
    "results.append(extract_full_metrics(y_test, y_pred_svm_glove_int, \"GloVe\", \"SVM\"))\n",
    "\n",
    "# 6. Random Forest on GLOVE ONLY\n",
    "y_pred_rf_glove = rf_glove.predict(X_test_glove)\n",
    "results.append(extract_full_metrics(y_test, y_pred_rf_glove, \"GloVe\", \"Random Forest\"))\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "display(results_df)\n",
    "print(\"\\nBest Performance Summary:\")\n",
    "best_row = results_df.loc[results_df[\"Weighted (F1)\"].idxmax()]\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Best Model:\n",
      "\n",
      "Data Preprocessing: \n",
      "    Rationale: Since punctuation and hashtags and mentions can sound aggressive or \n",
      "    feel like patterns showing sexism, minimal preprocessing was done to keep\n",
      "    the original features. URLs were removed as they are unknown unless we went in the site and checked\n",
      "    \n",
      "    Steps:\n",
      "    - Lowercase to keep things consistant \n",
      "    - took out URLs \n",
      "    - Whitespace normalization \n",
      "\n",
      "Feature Engineering: \n",
      "    We combined three feature types to capture different patterns of sexism:\n",
      "    \n",
      "    1. TF-IDF (1800 features): \n",
      "       - Captures word importance via inverse document frequency\n",
      "       - Bigrams (1,2) to capture common sexist phrases \n",
      "       - Chi-squared feature choosing to keep most discriminative terms\n",
      "       - min_df=3: this means a word must appear in 3 documents to be counted, ignors tpyos and such\n",
      "       - max_df=0.85: removes overly common words that don't indicate anything\n",
      "       - sublinear_tf=True: log scaling keeps extremely common words from dominating\n",
      "    \n",
      "    2. GloVe embeddings (100 features): \n",
      "       - makes synonyms match in importance as vectors\n",
      "       - Mean pooling combines word vectors into sentence vectors\n",
      "       - Complements TF-IDF by understanding synonyms and context\n",
      "    \n",
      "    3. Custom features: \n",
      "       features that matter a lot that the first two were missing:\n",
      "       - Derogatory word count\n",
      "       - Female pronoun to total pronoun ratio\n",
      "       - Checking for woman/women/girl/female/lady to weigh more\n",
      "       - Commanding language should/ must/ need/ have to\n",
      "       - Intensity markers, Exclamation marks \n",
      "       - Negative descriptors, Insulting adjectives \n",
      "       - Text length as a feature, scaled logorithmically\n",
      "    \n",
      "    Final feature vector: 1907 dimensions \n",
      "\n",
      "Model Selection and Architecture: \n",
      "    Ensemble approach (merging several models) to reduce overfitting and make generalization better:\n",
      "    \n",
      "    - Base model: Logistic Regression \n",
      "    - Ensemble: 4 models with different regularization strengths\n",
      "      * C values: {2.0, 3.0, 4.0, 5.0} for diversity in decisions\n",
      "      * Lower C = stronger regularization so less overfitting\n",
      "      * Higher C = weaker regularization so more nuance \n",
      "    - Average probability predictions \n",
      "    - Class weight {0: 1, 1: 1.5}: Penalizes misclassifying sexist examples more\n",
      "\n",
      "Training and Validation: \n",
      "    - Stratified 5-fold cross-validation \n",
      "    - Cross-validation during hyperparameter search prevented overfitting\n",
      "    - Threshold optimization: Tested 0.40-0.65 to find best threshold,\n",
      "      * a higher threshold meant that it would be more lax on deciding on sexist\n",
      "\n",
      "Evaluation and Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not sexist     0.8727    0.9037    0.8879       789\n",
      "       sexist     0.7175    0.6498    0.6820       297\n",
      "\n",
      "     accuracy                         0.8343      1086\n",
      "    macro avg     0.7951    0.7768    0.7849      1086\n",
      " weighted avg     0.8303    0.8343    0.8316      1086\n",
      "    \n",
      "    Confusion Matrix:\n",
      "    - True Positives : 193 correctly sexist\n",
      "    - False Negatives: 104 sexist examples missed \n",
      "    - False Positives: 76 non-sexist flagged as sexist \n",
      "    - True Negatives: 713 non-sexist correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"Summary.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)\n",
    "\n",
    "summary = \"\"\"Summary of Best Model:\n",
    "\n",
    "Data Preprocessing: \n",
    "    Rationale: Since punctuation and hashtags and mentions can sound aggressive or \n",
    "    feel like patterns showing sexism, minimal preprocessing was done to keep\n",
    "    the original features. URLs were removed as they are unknown unless we went in the site and checked\n",
    "    \n",
    "    Steps:\n",
    "    - Lowercase to keep things consistent \n",
    "    - Took out URLs \n",
    "    - Whitespace normalization\n",
    "\n",
    "Feature Engineering: \n",
    "    We combined three feature types to capture different patterns of sexism:\n",
    "    \n",
    "    1. TF-IDF (1800 features): \n",
    "       - Captures word importance via inverse document frequency\n",
    "       - Bigrams (1,2) to capture common sexist phrases \n",
    "       - Chi-squared feature choosing to keep most discriminative terms\n",
    "       - min_df=3: this means a word must appear in 3 documents to be counted, ignores typos and such\n",
    "       - max_df=0.85: removes overly common words that don't indicate anything\n",
    "       - sublinear_tf=True: log scaling keeps extremely common words from dominating\n",
    "    \n",
    "    2. GloVe embeddings (100 features): \n",
    "       - Makes synonyms match in importance as vectors\n",
    "       - Mean pooling combines word vectors into sentence vectors\n",
    "       - Complements TF-IDF by understanding synonyms and context\n",
    "    \n",
    "    3. Custom features (8 features): \n",
    "       Features that matter a lot that the first two were missing:\n",
    "       - Derogatory word count\n",
    "       - Female pronoun to total pronoun ratio\n",
    "       - Checking for woman/women/girl/female/lady to weigh more\n",
    "       - Commanding language: should/must/need/have to\n",
    "       - Intensity markers: exclamation marks \n",
    "       - Negative descriptors: insulting adjectives \n",
    "       - Text length as a feature, scaled logarithmically\n",
    "       - All-caps word count\n",
    "    \n",
    "    Final feature vector: 1908 dimensions (1800 + 100 + 8)\n",
    "\n",
    "Model Selection and Architecture: \n",
    "    Ensemble approach (merging several models) to reduce overfitting and make generalization better:\n",
    "    \n",
    "    - Base model: Logistic Regression \n",
    "    - Ensemble: 4 models with different regularization strengths\n",
    "      * C values: {2.0, 3.0, 4.0, 5.0} for diversity in decisions\n",
    "      * Lower C = stronger regularization so less overfitting\n",
    "      * Higher C = weaker regularization so more nuance \n",
    "    - Average probability predictions \n",
    "    - Class weight {0: 1, 1: 1.5}: Penalizes misclassifying sexist examples more\n",
    "\n",
    "Training and Validation: \n",
    "    - Stratified 5-fold cross-validation \n",
    "    - Cross-validation during hyperparameter search prevented overfitting\n",
    "    - Threshold optimization: Tested 0.40-0.65 to find best threshold\n",
    "      * A lower threshold means it's more sensitive to detecting sexist content\n",
    "      * A higher threshold means it's more strict/conservative about calling things sexist\n",
    "\n",
    "Evaluation and Results:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "   not sexist     0.8727    0.9037    0.8879       789\n",
    "       sexist     0.7175    0.6498    0.6820       297\n",
    "\n",
    "     accuracy                         0.8343      1086\n",
    "    macro avg     0.7951    0.7768    0.7849      1086\n",
    " weighted avg     0.8303    0.8343    0.8316      1086\n",
    "\n",
    "    Confusion Matrix:\n",
    "    - True Positives: 193 correctly sexist\n",
    "    - False Negatives: 104 sexist examples missed \n",
    "    - False Positives: 76 non-sexist flagged as sexist \n",
    "    - True Negatives: 713 non-sexist correct\n",
    "    \n",
    "    Final Performance: Weighted F1 of 0.8316 exceeds 0.82 target\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
